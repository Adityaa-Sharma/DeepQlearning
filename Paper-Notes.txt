Difference in the data related assumtions in the deep learnning and  RL:
    -> DL assumes the IID(independent and the identical data), like each data pt. is separate and all of them follows same probability distribution.
    -> whereas in Rl , data is correlated , i.e each stae depend on previous state, and state also depends on action etc.
    
Bellman equation:
    -> It is based on the optimality.
    -> It states that the value off the action -pair Q(s,a) is equal to the immediate rewanrd and the discounted best possible value of t
        future state.
            Q*(s,a)=E[r+Y maxQ*(s`,a`)|s,a]
            where,:
                -> s` is the state agent transition to.
                -> a` is the action agnet ccan take
                -> Q*(s`,a`), best possible Q value agent can have.

Why this computation did not work on Atari Games:(Q table lookup)
    -> No Generalization:
        -> The algorithm does not infer similarities between different states.
        -> If two states s1 and s2 are similar , still it will learn them separately as they are treated as completely new state.
        -> i.e slow training

        Eg.
        ->If the ball is slightly left of the center, it should behave almost the same as when it's slightly right.
    
    -> Huge State space:
        -> eg. 3*3 tic tac toe have only, 3^(9) states(action->x,o,empty)
        -> in atari it can go upto (action)^(84*84) for gray scale  and for RGB -> (actions)^(210*160)
    
    -> Data Correlation
        -> this breaks the IID principle for stable training principle.
    
    -> Memory inefficiancy:
        -> We cannot store huge sate action space, memory allocation will be huge.


Functionn Approxmators( Q(s,a,theta) ~ Q*(s,a)) 
    -> here theta is teb weights of the NN , which is used as the non-linear function aprroximator.
    -> Loss of the approximator.
            L(i) = E_{s, a} [(y_i - Q(s, a; Î¸_i))^2]

                where, y_i = E_{s'} [r + Î³ max_{a'} Q(s', a'; Î¸_{i-1})] (Traget value)
    Note:Basically we are calculating the loss between teh current state action value function with given s,a and the predicted 
        state action function given the transitioninig state (s->s`)(from the predicted network)

    Note: weights from the previous  iterationn ,i.e i-1 are kept constant while calculating the gradient.
        
        gradients: âˆ‡_Î¸ L(i) = E_{s, a} [(y_i - Q(s, a; Î¸_i)) âˆ‡_Î¸ Q(s, a; Î¸_i)]   (w.r.t i)

        -> SGD is used while calculating this , as it is not feasible to sum everyhting for this much large stae space.

    Behavior distribution:
        ->p(s,a) is the probability distribution over sequences s and actions a.
        -> The behaviour distribution refers to how the agent samples experiences based on its current policy.
    
    On-Policy and Off-Policy:
        On Policy->No exploration (SARSA)
        Of Policy-> Exploration (DQN)
        
        How exploration??
            e-greedy approach:
                Ï€(a | s) =  
                            {  
                            1 - Ïµ + (Ïµ / |A|),  if a = argmax_a Q(s, a; Î¸),  
                            Ïµ / |A|,  otherwise  
                            }

            -> Here, with probability 1-e , agent will pick the best action(exploitation)
            -> And with the probability e , it will take random action (exploration)
            -> During training, we have to decrease the e, as to reduce the exploration.

    Replay Buffer:
        ->The updates in the Q tableonly takes the most recent experiences, leading to the high correlation between the 
        consecutive training sample and which makes the training unstable.

        ->At time t, the agent:

            - Sees state s_t (game screen).  
            - Takes action a_t (move paddle left).  
            - Receives reward r_t (+1 if hits ball).  
            - Enters next state s_{t+1}.  

            This experience (s_t, a_t, r_t, s_{t+1}) is stored in memory.
        
        -> Instead of training on only the latest sample, DQN randomly samples a mini-batch from replay memory.
        -> Breaks correlation between consecutive frames.

    Stacking of the states:
        -> In image-based RL (e.g., Atari Pong ), the environment provides only the current frame.  
            - A single frame lacks motion information (e.g., ball direction).  
            - So, we stack the last 4 frames together to form a state.  

                S_t = (x_t, x_{t-1}, x_{t-2}, x_{t-3})  

            This allows the agent to infer motion and velocity!
    
    Model Architecture:
        -> Images were of 210 x 160 pixel images, to reduce complexity ddownsamole them to gray scale 110 x 84.
        -> futher downsampliing for GPU optimization 84 x 84.

        -> Neural network Architecture:
            Two Approaches:
                1) Both state and action as input:
                    Q(s,a)=f(s,a:theata)
                -> The network outputs a single scalar Q-value for that state-action pair.
                -> Each action requires a separate forward pass!
                -> Inefficient for the large action space like the atari games.

                2)Only state as input:
                ->Instead of taking both s and a  as inputs, the network only takes the state as input.
                -> The network outputs a separate Q value for each possible action.
                    Q(s,a)=f(s,theata)_a
                -> Only ONE forward pass is needed to get Q-values for all actions!
                -> Network will output nodes as the number of the actions.(important)
                -> In one formward pass we can get all the Q value of all the action , from that select the max Q value from this.

                ->Each output node corresponds to:  
                    Q(s, a1), Q(s, a2), Q(s, a3), ..., Q(s, an)  

                    ->The action with the highest Q-value is chosen:  
                                a* = argmax_a Q(s, a)
        
        Pre-Processing:
            conv1= 4(stack), 16, kernel_size=8, stride=4
            conv2= 16, 32, kernel_size=4, stride=2 
            RELU
            FC1= 32 * 9 * 9, 256
            FC2= 256, num_actions

        
        Reward Clipping:
            -> Original problem:  
                    Different games have hugely different score scales. 
                Example:  
                    - Pong : Scores range from -21 to +21.  
                    - Breakout : A single hit could give 10+ points.  
                    - Ms. Pac-Man : Scores can go into the thousands.  
                
            -> Solution:  
               -> Instead of using actual rewards, they clip rewards: 
               ->r =  
                        {  
                                +1, if reward is positive  
                                -1, if reward is negative  
                                0, if reward is zero  
                        } 
            
            -> stable training as gradients are normaised.


        -> e decay information:
            -> Trained for 10 million frames.
            -> 1---> 0.1 in first million frame than fixed to 0.1 for later frames.
        
        -> Frame skipping:
            -> k=4, that agent will selct action on ecverry 4 th frame rather that on every frame, reducing the computaion and can play more games.
            -> Same action is processed in th skipped frames. like at first frame i have choosen left, then on 2,3,4 left will apply and on 4 new action will be taken.
        
        -> Note: An episode starts when the agent enters the environment and ends when it reaches a terminal condition (e.g., winning, losing, or time running out).


        -> Evaluation Metrics:
            -> Epoisodic Reward.
            -> Predicted Q values: stable increasse-> stable training/learning.
                -> Issue: Q -value overestimation

            why Q value is better metric than the reward:
                -> Episodic reward the prone to noise, and random events.
                -> Q value is stable and not pronne to randomness as we extract the states randomly before the training only.

            How Predicted Q learning metric is calculated:
                Step 1: Collect a Fixed Set of States (Before Training)  
                ðŸ”¹ Before training starts, they run a random policy and store a set of states.  
                ðŸ”¹ These states are fixed, meaning they are the same for every evaluation.  

                Step 2: Track the Maximum Predicted Q-Value for Each State  
                ðŸ”¹ At different points during training, they:  

                    1. Feed these fixed states into the Q-network.  
                    2. Compute the maximum predicted Q-value for each state.  
                    3. Average these Q-values across all stored states.  
        